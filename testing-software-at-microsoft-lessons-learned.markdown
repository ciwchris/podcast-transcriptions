[Alan Page: Testing Software At Microsoft â€“ Lessons Learned](http://joecolantonio.com/testtalks/44-alan-page-testing-software-at-microsoft-lessons-learned/)
==========================================================

Released: March 8, 2015

Host: Joe Colantonio

Guest: Alan Page

Joe Colantonio: So, in regards to your book How We Test Software At Microsoft, how much has changed since that book and now?

Alan Page: A lifetime. In 6 or 7 years since I first started writing it, or so. I don't even know where to start, how much is different. A lot of the techniques in there described, are things which are still done they're just done often done differently. We have a lot of, what I'll call unified engineering teams at Microsoft, eventually they'll just be called engineering teams. We have a lot of teams where there aren't specific rules for testers. So those activities we talk about, looking at some of the technical things around test case design, or functional test design, I don't like the word test case that much, functional test design, or coverage, those are all still done but less so by the test team and more as part of the sort day to day work everyone on the team. There's also a much huger emphasis on, I think I wrote one chapter, I remember I wrote one chapter on customer feedback systems where I talked about things like are crash reporting systems and what we used to call squim, stands for sqm, which was a way to get data from customers usage patterns. But what we did 6 or 7 years ago compared to now is nothing. Today we're heavily dependant on what we call telemetry remote, remote metrics from customers. For customer usage, internal beta usage, etc, we get that in almost real time in a lot of cases. Very small delay versus what we used to get, a day or two or three days later and look at an aggregate. And also there's a lot more emphasis on how to analyze that data, both for business intelligence, to know what's going on, and then looking for insights in more the data scientisty part of the world. So that's probably one of the biggest shifts. My keynote at Star, to loop back to my plug, is talking about all these things. How much is different and what's changed. So those are probably the biggest things. Who does the activity is not necessarily the SDET. And in fact that first portion of the book that talk about the structure of the company and the roles of what we call the software engineer and test, those are hugely changed, mainly because a lot of the teams don't have testers. In fact, although it's hard to describe what a tester does anymore, you know. I've talked a lot about generalizing specialists. I have a specialization in testing and quality and how software quality is built, but I'm really more of the craftsman of the team, in most cases, then the tester. Are developers are actually writing pretty good tests on our team. My job is sort of looking end to end and making sure I know where the wholes are and make sure we're actually approaching engineering at a proper way to make quality software and hopefully build something that is fun and usable for customers. The short answer is that it has changed a lot and, somethings haven't changed, but a lot has changed.

Joe Colantonio: There used to be a statistic that there was one tester for each developer at Microsoft. But now it sounds like that not how it is. It almost sounds like Microsoft did exactly what some of the other companies have been doing, what they call shift left. So you're seeing most of your developers now are, the testing is part of the actual development process, it's not something that is done at the end of the development life cycle.

Alan Page: Correct, that's the staged part, even on our very formal like desktop, big thick application teams, the functional testing, like does this work, for years and years my job as a tester at Microsoft was basically to write unit tests. I'd get code and I'd blow on it lightly and it would fall over. Those days are rare. It's actually harder to find bugs in our code, they're still bugs, but the expectation that functional bugs are found during the coding process, before it ever gets checked in, before it ever makes it into a branch that anyone would care about looking at. So that part of shift left has happened a lot. There's still, I think there's plenty of room for people like me and a lot of lifetime or quality test folks to look at reliability, performance, security, end to end, integration especially. Windows, doesn't matter how functional every little bit of code is, when you put it all together you need it to be both usable, which is hard, I've heard this many times in the past that Microsoft makes wonderfully functional products that work nicely that my wife can't use. So there's a lot of room for looking at how things fit together end to end and making some good decisions there. But there's also things, as you know, beyond just usability pieces that don't quite fit together, when you put the whole system together, that's where are test specialists really fit in. I don't know what the ratio is across the company anymore, I'd probably give you two different ratios. There'd be the ratio of how many people are called testers versus how many people are called developers. Then there's the ratio how many people are in a test specialist kind of role versus people who write code all the day. And I'd have to guess at both of them. My title now is just Software Engineer. Although I spend most of my day doing very different tasks than the Software Engineer in the office next to me does. I think that's okay, I think that's healthy. By the way no matter which way I slice it from those two different angles, neither one is close to that one to one we used to have. But that one to one was necessary because for a long time the role of the tester was this weird, I can be open here, it was a weird sort of relationship where, what's the word I'm looking for, co-dependent relationship, where the developer would give the tester the code expecting them to find bugs and then the tester would validate themselves by finding bugs and giving it back to them. Which made this nice little dependency, you have your little dev partner and they give you code and you give them bugs and you go back and forth and it's great. But when you think about it is really wasteful and inefficient. Those days are gone, thank goodness.

Joe Colantonio: I thought I read somewhere though that you said it takes a certain kind of thinking, for a tester to write good tests, so how did Microsoft make that transformation then, that they feel confident enough that their developers also are going to write good tests.

Alan Page: It depends on the definition of granularity of tests. I think that developers can write, almost every developer I know can write very good functional tests. Cause they have an intent. This should do this, I can write a test to make sure this does this. I think they're very good at that. Not great. And I actually spent, on this team I'm on a year ago made this shift left where developers were going to own all the functional testing, I probably spent 3 months straight reviewing code about 2 to 3 hours a day of all the check-ins for our team, just reviewing the test code and giving feedback. And, I don't need to do that anymore because our developers, for the most part, write pretty good tests and they kind of get it now. All the things we learned over the years I tried to ram down their throats in about 2 to 3 months, tests should be diagnosable, if it fails it should mean something, getting rid of the flaky tests etc, etc. So that part, they just need a little bit of training in, they can do pretty well. There is, however and I'm sure I've mentioned this before, great testers from those that investigate the software and kind find the holes in the larger system are good system thinkers, they see the system as a whole they can figure out how piece A should integrate with piece B, what that should look like, when it's working and when it's not working. And they also have some specialties around non functional areas, or whatever. But those kinds of people are still employed and we still, like me, we find areas to specialize, like in helping developers write tests, but we're also, the reason I'm employed is cause I'm here to look at big picture of how things fit together and make sure we're doing the right thing, which is more of that tester mindset. I'm a big fan of concept, generalizing specialist, specializing generalist. I think the bulk of our team, the development team, they're day to day role is to implement code, test that code, make sure it works and integrate it into the system. There's a smaller portion of us who look at how that's all fitting together and finding the wholes in that. It's a different shift and a different approach and hopefully there's enough of the what we learned in the old world around that we don't let things fall through the cracks, but it seems a lot more efficient to me, just to do it that way. But anyway to answer your question, functional test devs can be very good at writing, there's no reason they can't write their own unit and beyond unit functional tests. Actually on our team we call them inner loop and outer loop and I call them short tests and long tests. And our development teams own writing all of the short tests. And also those things are very biased toward verification, pass/fail. The kind of tests that what we call our quality team writes, much smaller team, we often do the data analysis work, but we write longer tests that are more investigative. You know a perf test may not pass or fail. In fact unless your setting a threshold wouldn't pass or fail but gives you information, let's you know about, that you have to interpret. Reliability tests are often the same way. Beyond crashes are their error paths getting hit, which ones we investigate if there is a problem or not and how to diagnose that and use that information in the future. But a lot of the tests people like me write are tests which don't have a pass/fail criteria at the end. They give us some information about the state of the system that we can use in context with all of are other things that are giving us information about the state of the system to let us know, to give us some informed decisions on how the software is behaving.

Joe Colantonio: When you say tests, what kind of tests are these? I know in your book you say 95% of all automation against a GUI is a waste of time. So I'm just curious when you say testing what kind of testing are you doing.

Alan Page: I should probably clarify the GUI thing before I make everybody mad. 95% of the time 95% of the people will write bad GUI automation just because it is a very difficult thing to do. If you have well designed software it can be easier, there are all kinds of things you can do that will make it easier for you to be successful, but it's a scary place to be because there's just a lot of fragility there, and I don't like flaky tests. It's funny, I went to the Google Test Automation conference, I'll get to your question eventually, I went to the Google Test Automation conference last fall and I think, not every single, but almost every single speaker, at least at one point brought up the concept of flaky tests and what to do about them. So I was joking around and calling it, next year we should call it the Flaky Test Automation conference. And those aren't even all GUI tests, but it's sort of what we've learned, and those of us who've written automation over the years is that it is very difficult to write trustworthy automation where if it fails we go, it's a product bug automatically. We go look at it and go, let me see if it's a test bug first, okay, it's not a, it's a product bug. We do silly things like, the test failed, I'll run it again, okay it passed, it's fine. I used to bang my head against the desk, no, no, no. Alright, so what kind of automation do we write. For the long tests, those outer loop tests, we're looking at things like, let me go back to XBox and talk a little bit about there. That will be easier to keep myself out of trouble. So we may have a long test that puts a new build on the XBox and then launches some apps that run in one of the operating systems, XBoxes have three operating systems, and then have it switch. Sorry, I'm talking as I think which rarely works out. We may try a stress type test, may try to stimulant a few months of actions in a day, so we're going to start game, stop game, put games into that windowed state, we're going to switch over to apps in the other operating system, we're going to do a search, we're going to initiate a Skype call, etc. So those kinds of things which we can all do at a layer below the GUI and be pretty accurate on knowing that when we call the actions its going to work. We're going to look for system reliability issues. In addition to that we did have a nightly stress mix which is distributed that we would write and have all kinds of crazy things happening to the file system, etc. Trying to find areas where the XBox or the operating system may crash, very similar to what Windows does. We're going to write tests that capture frame rate over time, while different things are going on to make sure they meet the specs we thought they would as far video playing or frame rates for games etc. Things like that. Where as the functional test the developer may write make sure the video plays and that there are no dropped frames in the first 30 seconds. Because those functional tests the developer team usually writes are short, often tend to be quick, because they don't want to have to wait an hour to decide if the change they made is good to check in, or longer. Okay, it looks like I didn't cause any regressions, we're going from there. Another area where we wrote a lot of investigated tests was around the Kinect device. We generated a lot of simulation around objects in the room so we could try to simulate 4 players, the dog walking through, the plant over here with this kind of lighting, that kind of lighting, this size room, etc, to try to find areas, what we call skeleton tracking wasn't working as well as we wanted it to. So things like that, things that, a little more in depth and long, we'll write automation for those because doing those manually is very difficult and time consuming. So the kind of tests we're writing, in short, after that long answer, is things that may take a lot longer to run, things that give us more information about the system, there are some examples in there that will hopefully help.

Joe Colantonio: So, I guess that almost sounds like when say automation you're not against automation it just sounds like you're more for lower level automation, like API automation.

Alan Page: Yeah, and to be clear, maybe a better way to point that, is that I'm against automation that attempts to simulate what that user does. To me that's just silly. And I think that's what unfortunately what I see a lot of teams do when they adopt some tool is, the user is going to do this and this and this and I'll make sure they can do this on every build. You can do that a little bit for a website using Selenium to make sure that as a quick verification test, but doing anything complex, I think the more complex you get in those scenarios the more prone you are to error. I'd rather use automation, or as many people call it that programming assisted testing just to make it easier for me to do complex actions reliably.

Joe Colantonio: I guess we're doing them wrong where I am. We're doing Behavior Driven Development.

Alan Page: I love BDD.

Joe Colantonio: Okay, but what we're doing though, when they do BDD, and even though they don't listen to me, I say we should do more low level tests, they make it all UI level, because they say, we're a medical device, we need to simulate exactly what are user is going to do, and they don't understand that even if the test is at a GUI it's not exactly what a user is going to do because it's calling usually a JavaScript or something, it's not even being rendered in a browser and sometimes we're doing hooks into those JavaScript calls. So how do you get around that, or how would you try educate someone that doesn't understand exactly what should be automated or at what level it should be automated at?

Alan Page: You're never going to simulate the user. It shouldn't be hard, but it's a hard hump to get over often. I think you look for data to help backup where it doesn't help. Every time you get to a point where either the automation passes and the UI is all screwed up, or the automation fails and it should have passed because someone change the UI and didn't change the automation. All of those things give you data that helps you make a shift. Look, we have these things that passed that should have failed, because we didn't actual eyes on this, we relied on the automation. And these things failed and should have passed because the automation was flaky because we're trying to manipulate the UI directly. All those things give you data that help you make a shift. What I prefer to do, if I were in charge, I would make sure that the application can be automated without manipulating the UI. You know, model view control pattern is great for this, finding a way to implement the underlying functionality. Because what you want your automation to do is verify functionality, especially at that BDD level. So is the functionality working. Can I still, I don't know your device, can it still take a pulse, can it still detect oxygen, are the sensors still working, etc. That's the important thing for automation to figure out. And then, often even 5 minutes using the UI directly can tell you whether these button aren't lined up at all, or the screen turns pink when I press this button. Those are things automation can't necessarily detect, those are great things for the human eye to look at. So I'd rather split those up. And then again, automation works, if you can manipulate a level below the GUI then I'm feeling a lot better about writing automation. You can still call it UI automation but I know that you are implementing something a little bit more reliable. And you're probably getting more out of it then what you want.

Alan Page: I want to go back to BDD for a minute, because I'm a big fan. I did a little bit of BDD work on XBox. We didn't have a great way at the time to get the gherkin stuff over the wire to the XBox, but what I found out, and I'm not going to give a huge BDD explanation, people can look it up and figure it out, but when I worked with some developers on our team on Behavior Driven Development, the huge benefit came not from the automation at the end, that comes out of , I assume you use some sort of gherkin language for your BDD, so just in thinking up front about what the behaviors were and describing what those are, that was the huge win for me. Automation is some icing at the end, you get some automation out of that at the end, but I found we got much better implementation, much less straying, much more accurate, much more often working right the first time, just from those developers thought about the behaviors and tried to describe them before they went and implemented the code. At a very minimum, for those curious about BDD, just do that first, and then if you get some automation out of it in the end that's fantastic too, to verify things. But in my experience it's been a great way to influence good software design, it's describing those behaviors upfront before implementation starts.

Joe Colantonio: I totally agree, and that's how it should done, but I guess I work for a very dysfunctional projects. The benefit is the collaboration, the talking before you even develop anything. That's going to save you time in the long run. Not these flaky, thousands of automated tests, which are going to be a nightmare to maintain. It's the actual discussion that you're having upfront that's going to help with the reliability later on, and to really uncover bugs before it's even developed. So I definitely agree with you, I love BDD from what it's really for, but from my experience I see teams that use it almost just for an automation framework, rather than, we're going to get 100% automation and our scenarios are our requirements, so our requirements are going to have an automated test, and we're going to be 100% automation and everything is going to be happy.

Alan Page: This is where that tester mindset helps out, because you and I we see the big picture and how the system fits together, and often the people that don't have that tester mindset see, okay here's a great tool we can use to achieve this, but no you have to look at it in the big picture. There was just an article, I don't know if you saw this, like a week ago and a bunch of people have commented on it, this guy wrote an article something called Scrum Should Die in a Fire, and he basically had a bad experience on dysfunctional teams implementing Scrum, what they called Scrum in strange ways, and it's exactly what you'd expect, of course if that was your view of Scrum, you go yeah this does suck. But the same thing with BDD or any, you can call it automation, any tool your throw at something, if you look at it blindly as this tunnel vision kind, this is the solution that's going to do this for me, you might end up in a place where it's not doing at all what you expected it to do for the product overall. I think your BDD story fits that template pretty well.

Joe Colantonio: It's good to hear that you're using BDD and that it is helpful to your team. That's good to know.

Alan Page: Yeah, and again, if you can just make people walk through that Give, When, Then, it's a great little mini spec-lt, I call a very small spec at spec-lt.

Joe Colantonio: So in your experience is there anything you see over and over again that most people do in test automation they're doing wrong.

Alan Page: Most often in my case, cause I've worked a lot in operating systems, it's relying on behavior of the underlying software that they don't fully understand. That's probably one of the biggest ones I've seen. Like assuming something is, this button will always be 24 pixels wide. Which you think, that's dumb. Sometimes it's just straight out programming errors, because testers make them as often, at least as developers. You use less than when you meant to use greater than, it worked fine for awhile, or should have less than equal. A quick side note on that, one of the bombs I try to drop on people when they talk about flaky tests, if you have tests that are failing that should be passing and you're okay with that, are you okay that you have tests that are passing that should be failing. Those are harder to find. So it is important to have that reliability on both sides. And I think until you trust that your failing tests are not flaky you can't trust your pass rate either. So yeah, that can be scary when you think about it. And, again, to be fair, not all classes of things that cause flakiness in failing can be applied to passing tests, but it's worth thinking about. So that's why you want to put that rigor, or at least craftsmanship is a better word, into all your tests, the more I can trust my failed tests, the more I can trust my passing tests, and that's pretty important. Another big reason for failure is relying on behavior that you assume will always be there, but it's not. Relying on networking existing, because it may not. Plenty of people use computers not connected to a network, even today. So being able to anticipate those things. It really comes down to assumptions. There's general programming errors and then just failed assumptions. I probably can look at some code and think of some more but those are probably the biggest that come to mind.

Joe Colantonio: I think you brought up a good point that if you don't have confidence in your tests that are passing that that's an issue, so do you recommend when people are writing automation that they follow the same TDD type practice where they make it fail first to get confidence that the test is really going to fail if it doesn't meet what they expect it to do.

Alan Page: It's interesting that you mention that. Full TDD maybe not. It gets to that thing, who tests the tests, who tests the test that tests the tests. But there are a couple things as a tester I do that I follow very close to that. Yeah, I want to see, can I make this test fail. Either by manipulating the operating system or just changing a comparison operator. Just to make sure that I know what it looks like when the tests fails, then I can diagnosis from the failure. A lot of the feedback I give to our developers when writing tests was it's not enough to just fail, there's no official metric on this, but mentally I track mean time to diagnosis. Which is when a test fails how long does it take me to figure out why. And we have testers who sometimes spend half a day trying to figure out why their test failed. And I'm little off your original question here, but when a test fails, I think if you have to go hook up a debugger or run the test again you've lost. You've lost the battle. I think you've won the battle when a test fails and you look at the log and within a minute or 2 or less you can say most likely it's this. To me it sort of common sense, at least after 22 years it's common practice. That's my biggest thing. But yeah, I do like to see the test fail, and often because I've worked on operating system a lot of my life, I've worked with kernal debuggers a lot and I will do what I've called in the past exploratory debugging when I write tests. I'll write my tests, and you can do this at application level as well, I will, right where my test determines whether it's passed or failed, or the sort the money part of the test, I'll set a break point there, and I'll want to look at the operating system, what the variables look like, what's going on when it passes or fails so I get an idea of things I may want to log, it gives me some ideas there, there may be an error path in the code that I'm testing that I want to exercise, I can force the debugger to go over there, look through it, that gives me more test ideas. Those sorts of things. But to answer the original question, I wouldn't necessarily go full on TDD for test development, but I do want to see what my tests look like when they fail while I'm writing them to make sure I understand what that looks like and how that many influence the tests I'm writing.

Joe Colantonio: I think you brought up an excellent point that it does sound like common sense but in my experience it's not and that's when a test fails that you're just rerunning it to find out did it pass and if it did then don't worry about it. You need know when it fails to be able to look at logs really quickly to know okay it failed because this assertion didn't match or something. I forget the name of the metric, I'll go back and write it.

Alan Page: I call it the mean time to diagnostic, but I may call it something different next time.

Joe Colantonio: I think that's a killer metric, that's one of my questions, are there any metrics you've found helpful in your career that actually are helpful and not just manager driven and I think that would be one example.

Alan Page: Yeah, but those are sort of personal metrics. I'm no going to track that for Joe's tests. How long do you take to figure out diagnostics, how long does Susan take, therefore Susan writes better tests. That's not necessarily true.

Joe Colantonio: Well in general though if you have a suite of tests that are failing and you're not there someone needs to look at those tests and see how long it takes to find that information out. I think at a high level we should have good logs at least for that purpose.

Alan Page: Yeah, I'm big on efficiency, the one thing I took away from my Myers Briggs that I did 15 years ago was that my particular type abhors inefficiency and I knew that beforehand but now at least I know why, sort of my personality type aligns and make me look at inefficient things and say WTF and try to find a more efficient way to do them.
